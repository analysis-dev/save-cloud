## Save: Software Analysis Verification and Evaluation

In this article we would like to introduce you a new cloud CI ecosystem called **SAVE**.
It's main purpose and main idea is to make the life of dev-tool developer easier.
Actually SAVE can be very useful for three types of developers: 
- who need a **Continuous Integration** system for a validation of their tools;
- who need a **Benchmarking Platform** to compare their tool with other tools;
- who need a **Validation Platform** to check their tool against a predefined standard;

As a bonus in future SAVE plans to provide a **Community Competitions** platform.

## Our initial motivation
Our team is focused on the creation of libraries and tools for developers.
And this idea came to us when we were developing an opensource Static Analyzer, called [Diktat](https://github.com/saveourtool/diktat).
During our development we found out that there is a lack of test frameworks that can be used for testing such kind of tools.
Each author of compiler or static analyzer reinvents his own test framework only for his purposes again and again.
We wanted to change this situation!

We checked multiple different opensource static analyzers and compilers, and the only tool that was useful for such kind of testing - was [LLVM LIT](https://llvm.org/docs/CommandGuide/lit.html).
All other frameworks have limited capability, small opensource community and were narrowly focused. 

LLVM LIT - is a testing framework for tools from LLVM ecosystem (Clang/LLVM backend/CSA/etc). 
But after some investigation of this library, we found some main disadvantages that even LIT has: 
- basically LIT - is a set of Python scripts and is not ready-to-go without extra installations;
- LIT was designed for compilers and has no functionality to extend the application logic; 
- as LIT was made by community system programmers, it has zero ecosystem and is just a simple cli tool;

Very important thing that we should mention is that testing scenario is very common for such kind of programming tools
(compilers, code analyzers, parsers, license scanners, etc):

1) Some source code (in a string format) is passed to the tool;
2) The tool as a black-box makes some internal logic;
3) Tool outputs some result: fixed code (for linters), code execution result (for compilers), warnings (for static analyzers),
some Internal Representation (for parsers), etc. All output data can be in a `string format`.

<img src="/info/img/Scenario.png" width="512px"/>

## SAVE-cli
### Command line app

Initially the above motivation brought us to the creation of [SAVE-cli](https://github.com/saveourtool/save-cli). 
It is a simple **native** command application that can be used for a functional testing of your tool.
We wanted to make a universal framework that can be used by anyone and remove all dependencies to particular eco-systems (JVM/Python/etc).
We understand that C++ compiler engineers usually do not want to install Java, Java developers of static analyzers do not want to install Python and so on.
So all that users have to do is to write tests in `SAVE format` and do not need to think about anything else.

We also wanted to make SAVE-cli as extendable as it is possible: we created plugin-like interfaces, so everyone can easily
contribute and add their plugin to SAVE. Right now out of the box we have `fix` and `warn` plugins.

<img src="/info/img/StaticAnalysisProcess.png" width="512px"/>

### Plugins
[Fix](https://github.com/saveourtool/save-cli/blob/main/save-plugins/fix-plugin/README.md) plugin is not so interesting,
as it simply runs provided executable on the initial test file and compares its output with an expected result.
By the way, to do this comparison, we use our own [diff library](https://github.com/petertrr/kotlin-multiplatform-diff).

The most interesting thing happens in [Warn](https://github.com/saveourtool/save-cli/blob/main/save-plugins/warn-plugin/README.md) 
plugin as we have prepared our own **DSL** for this case that can be used in the source code of test examples. This plugin reflects 
a common scenario for the validation of warnings generated by Static Analyzer or, for example, errors generated by Compiler's Frontend.

This is an example of such inline configuration of resources for this plugin:
<img src="https://user-images.githubusercontent.com/58667063/146390474-71e4921d-416b-4922-b2ea-894f71e491c3.jpg" width="512px"/>

As you can see - everything is very readable and user-friendly (we even have **regexes**, **multiline** warnings and **patterns**).
Also all syntax for this DSL is fully configurable in SAVE, so you can easily use proper syntax of comments in relation to your 
programming language.

### Resource detection mechanism

For SAVE-cli we implemented our own recursive resource detection mechanism. To make SAVE detect your test suites you
need to put `save.toml` file in each directory where you have tests that should be run.
These configuration files inherit configurations from the previous level of directories.

For example, in case of the following hierarchy of directories:
```text
| A
  | save.toml
  | B
    | save.toml
```

`save.toml` from the directory `B` will inherit settings and properties from directory `A`.

In SAVE we decided to make the following rule: **one suite - one save.toml config**. This means that if in your project 
you have a directory with tests, and you would like SAVE to detect them - you should put a `save.toml` file with at 
least basic information: suite name/description. Otherwise, this directory will be ignored.

Speaking about test resources - SAVE will treat all files with `Test` suffix as test resources and will automatically use configuration 
from `save.toml` file that is placed in the same directory (and inherited configuration):
```text
| A
  | save.toml
  | B <<< test suite
    | myTest.java <<< test resource
    | save.toml
```

## SAVE-cloud motivation

Of course the fact that we didn't have a **test framework** was not the only problem when we were developing our own Static Analyzer.
Each and every creator of static analyzers in the beginning of his development journey starts from the very simple thing: **types of issues** that his tool will detect.
This leads to a searching of existing lists of potential issues or test packages that can be used to
measure the result of his work or can be used for TDD (test driven development). In other areas of system programming such benchmarks and test sets already exists,
for example [SPEC.org](http://spec.org/benchmarks.html) benchmarks are used all over the world to test the functionality, evaluate and measure the performance of different applications
and hardware: from compilers to CPUs, from web-servers to Java Clients. But there are no test sets and even strict standards for detection of issues that can be found in
popular programming languages. There were some guidelines of coding on C/C++ done by [MISRA](https://www.misra.org.uk/), but there are no analogues of it even for the most popular
languages in the world like Python and [JVM-languages](https://stackoverflow.com/questions/6050618/is-there-a-java-equivalent-to-misra-c). 


There are only existing test suites at [NIST](https://samate.nist.gov/SRD/testsuite.php), but their framework and ecosystem remain very limited.

In this situation each and every new developer that reinvents his new code style or mechanism of static analysis each time **reinvents** his brand new test framework and writing test sets
that **have been written already thousands of times** for this particular programming language. Someone uses existing guidelines like [Google code style](https://google.github.io/styleguide/javaguide.html)
or using [PMD rules](https://pmd.github.io/). But in all cases a lot of time will be spent on reinventing, writing and debugging tests.

We decided that we need finally to change this situation and make an opensource product that contain the following components:
1) Universal Native test framework;
2) CI platform that would execute tests in Cloud;
3) Dashboards to visualize results;
4) Storage for the historical results of tests;
5) Regression and flaky tests detection;
6) Benchmarks Archive that can be used for certification or comparison of tools;

## What else we can get from SAVE-cloud?

While we were investigating existing testing tools in huge projects, like gcc or clang - we found out that these projects have
hundreds of thousands of tests. **More than 500.000 tests in each compiler!** Imagine, how slow will the CI processing be in such case.
We definitely need batching and parallelization mechanism using Cloud environment, especially when we know that all tests 
are usually encapsulated and isolated from each other. 

## SAVE-cloud

### Processing
So after the SAVE-cli was done, we started to implement our ideas in SAVE-cloud. We wanted these projects to be separated:
- SAVE-cli for local testing and checking configuration by user;
- SAVE-cloud for Cloud testing with SAVE-cli and storing of historical results;

So the logic is the following on the high-level:
1) SAVE-cloud provides a REST API or a WEB UI for the user;
2) User can select existing or upload his own benchmarks;
3) Processing triggers and orchestrates Kubernetes nodes where Native SAVE-cli executes tests in docker containers;
4) All historical results for exeuctions are saved in a DB for analysis;

<img src="https://user-images.githubusercontent.com/58667063/146387903-24ba9c91-a2a3-45e7-a07a-cb7bc388e4aa.jpg" width="512px"/>

### How it looks for the user
SAVE instance is deployed to **saveourtool.com**

<img width="512px" alt="image" src="https://user-images.githubusercontent.com/58667063/173469699-762d100d-9c7a-43e3-a76d-4f487856ddda.png">

<img width="512px" alt="image" src="https://user-images.githubusercontent.com/58667063/173469831-8ca547de-9db6-4c14-b8e2-4091c8a4af05.png">

<img width="512px" alt="image" src="https://user-images.githubusercontent.com/58667063/173470024-a7af3ac1-e792-48fa-80f5-5ecd6cfceb70.png">

<img width="512px" alt="image" src="https://user-images.githubusercontent.com/58667063/173470224-368e9118-6c75-454a-814d-bca367afb23d.png">

<img width="512px" alt="image" src="https://user-images.githubusercontent.com/58667063/173470482-6f70755c-8a53-4c51-8fca-821f72fd5924.png">

### SAVE API
Of course, SAVE-cloud also have a REST API and a Java library that can be used a client to this API. Using this API it can be easily integrated to various CI/CD platforms, like GitHub Action, Genkins, TeamCity and many other. You can read about the API [here](https://github.com/saveourtool/save-cloud/blob/master/save-backend/Backend-API.md).


## Conclusion
We think that SAVE in the future will become a standard for testing, benchmarking and standardization of dev-tools, especially
static analyzers and compilers, and SAVE-cloud will be used as an advanced and high-loaded Cloud Continuous Integration Platform.
Probably, in the nearest future it will even be used as a system for making contests related to Program Analysis, where standard 
committees will be able to propose their tasks and community will be able to participate in different challenges.

If you would like to support us, please give us a star on the GitHub:
- https://github.com/saveourtool/save-cli
- https://github.com/saveourtool/save-cloud


